=========================================
 Multiplatform programming with xobjects
=========================================

.. contents:: Table of Contents
    :depth: 4

Definition of a simple data structure class
===========================================

.. code-block:: python

    import xobjects as xo

    class DataStructure(xo.Struct):
        a = xo.Float64[:]
        b = xo.Float64[:]
        c = xo.Float64[:]
        s = xo.Float64


Allocation of a data object on CPU or GPU
=========================================

.. code-block:: python

    ctx = xo.ContextCpu()
    # ctx = xo.ContextCupy() # for NVIDIA GPUs

    obj = DataStructure(_context=ctx,
                        a=[1,2,3], b=[4,5,6],
                        c=[0,0,0], d=0)

Access to the data
==================

The object is accessible in read/write directly from python:

.. code-block:: python

    print(obj.a[2]) # gives: 3
    obj.a[2] = 10
    print(obj.a[2]) # gives: 10

Numpy-like access
=================

Xobjects arrays can be viewed as numpy arrays (or numpy-like on GPUs).

.. code-block:: python

    arr = obj.a.to_nplike()
    type(arr) # gives: numpy.ndarray

This object is a view and not a copy, which means that we can operate on the numpy arrays getting the result directly in the the xobject:

.. code-block:: python

    print(obj.a[0], obj.b[0]) # gives: 1.0 4.0
    a_nplike = obj.a.to_nplike()
    b_nplike = obj.b.to_nplike()

    # We use np array algebra
    a_nplike[:] = b_nplike - 1

    print(obj.a[0], obj.b[0]) # gives: 3.0 4.0

We can also use numpy methods, for example we can write:

.. code-block:: python

    obj.s = a_nplike.sum()

C kernels
=========

Writing the source code
-----------------------

We want to write a C function (running on CPU and GPU) that performs the product between ``obj.a`` and ``obj.b`` and places it in ``obj.c``.

.. code-block:: python

    src = '''

    /*gpukern*/
    void myprod(DataStructure ob, int nelem){

        for (int ii=0; ii<nelem; ii++){ //vectorize_over ii nelem
            double a_ii = DataStructure_get_a(ob, ii);
            double b_ii = DataStructure_get_b(ob, ii);

            double c_ii = a_ii * b_ii;
            DataStructure_set_c(ob, ii, c_ii);
        } //end_vectorize

    }
    '''

Explain vectorize...

Kernel description
------------------

We write the function description:

.. code-block:: python

    description = xo.Kernel(
        args = [xo.Arg(DataStructure, name='ob'),
                xo.Arg(xo.Int32, name='nelem')],
        n_threads='nelem')

Kernel description
------------------

We ask the context to compile the kernel:

.. code-block:: python

    ctx.add_kernels(sources=[src],
        kernels={'myprod': description})

Calling the kernel
------------------

The kernel can be called from python as follows

.. code-block:: python

    ctx.kernels.myprod(ob=obj, nelem=len(obj.a))


Inspecting the source code
--------------------------

Autogenerated code on CPU can be inspected by:

.. code-block:: python

    print(ctx.kernels.myprod.specialized_source)

It the chosen context is ContextCpu the generated specialized source is:

.. code-block:: c

    void myprod(DataStructure ob, int nelem){

        for (int ii=0; ii<nelem; ii++){ //autovectorized

            double a_ii = DataStructure_get_a(ob, ii);
            double b_ii = DataStructure_get_b(ob, ii);

            double c_ii = a_ii * b_ii;
            DataStructure_set_c(ob, ii, c_ii);
        }//end autovectorized

    }

If the chosen context is ContextCupy the generated specialized source is: 

.. code-block:: c

    __global__
    void myprod(DataStructure ob, int nelem){

        int ii; //autovectorized
        ii=blockDim.x * blockIdx.x + threadIdx.x;//autovectorized
        if (ii<nelem){ //autovectorized
            double a_ii = DataStructure_get_a(ob, ii);
            double b_ii = DataStructure_get_b(ob, ii);

            double c_ii = a_ii * b_ii;
            DataStructure_set_c(ob, ii, c_ii);
        }//end autovectorized
    }


If the chosen context is PyOpencl the generated specialized source is:

.. code-block:: c

    __kernel
    void myprod(DataStructure ob, int nelem){

        int ii; //autovectorized
        ii=get_global_id(0); //autovectorized

                double a_ii = DataStructure_get_a(ob, ii);
                double b_ii = DataStructure_get_b(ob, ii);

                double c_ii = a_ii * b_ii;
                DataStructure_set_c(ob, ii, c_ii);
        //end autovectorized

    }




